{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pymysql"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWDB4ShFiGH3",
        "outputId": "b195a47a-500c-4207-b20b-f8dddce0a371"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymysql in /usr/local/lib/python3.10/dist-packages (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nvIxlNJlLopU"
      },
      "outputs": [],
      "source": [
        "import pymysql\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect('example_2.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "cursor.execute('''\n",
        "CREATE TABLE users (\n",
        "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    reliability FLOAT\n",
        ")\n",
        "''')\n",
        "\n",
        "cursor.execute('''\n",
        "CREATE TABLE datasets (\n",
        "    id INTEGER PRIMARY KEY AUTOINCREMENT\n",
        ")\n",
        "''')\n",
        "\n",
        "cursor.execute('''\n",
        "CREATE TABLE categories (\n",
        "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    category_name TEXT NOT NULL,\n",
        "    dataset_id INTEGER NOT NULL,\n",
        "    FOREIGN KEY (dataset_id) REFERENCES datasets(id)\n",
        ")\n",
        "''')\n",
        "\n",
        "cursor.execute('''\n",
        "CREATE TABLE documents (\n",
        "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    document_text TEXT NOT NULL,\n",
        "    dataset_id INTEGER NOT NULL,\n",
        "    predicted_category_id INTEGER,\n",
        "    FOREIGN KEY (dataset_id) REFERENCES datasets(id),\n",
        "    FOREIGN KEY (predicted_category_id) REFERENCES categories(id)\n",
        ")\n",
        "''')\n",
        "\n",
        "cursor.execute('''\n",
        "CREATE TABLE annotatorLabels (\n",
        "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    document_id INTEGER NOT NULL,\n",
        "    category_id INTEGER NOT NULL,\n",
        "    annotator_user_id INTEGER NOT NULL,\n",
        "    FOREIGN KEY (document_id) REFERENCES documents(id),\n",
        "    FOREIGN KEY (category_id) REFERENCES categories(id),\n",
        "    FOREIGN KEY (annotator_user_id) REFERENCES users(id)\n",
        ")\n",
        "''')\n",
        "\n",
        "cursor.executemany('''\n",
        "INSERT INTO users (id, reliability) VALUES (?, ?)\n",
        "''', [\n",
        "    (1, 0.95),\n",
        "    (2, 0.80),\n",
        "    (3, 0.75),\n",
        "    (4, 0.90),\n",
        "    (5, 0.85)\n",
        "])\n",
        "\n",
        "cursor.executemany('''\n",
        "INSERT INTO datasets (id) VALUES (?)\n",
        "''', [\n",
        "    (1,),\n",
        "    (2,),\n",
        "    (3,)\n",
        "])\n",
        "\n",
        "cursor.executemany('''\n",
        "INSERT INTO categories (id, category_name, dataset_id) VALUES (?, ?, ?)\n",
        "''', [\n",
        "    (1, 'Sports', 1),\n",
        "    (2, 'Politics', 1),\n",
        "    (3, 'Technology', 2),\n",
        "    (4, 'Health', 2),\n",
        "    (5, 'Entertainment', 3)\n",
        "])\n",
        "\n",
        "cursor.executemany('''\n",
        "INSERT INTO documents (id, document_text, dataset_id, predicted_category_id) VALUES (?, ?, ?, ?)\n",
        "''', [\n",
        "    (1, 'The football match was thrilling with a last-minute goal.', 1, 1),\n",
        "    (2, 'The new policy has caused quite a stir among the citizens.', 1, 2),\n",
        "    (3, 'Innovations in AI are driving the tech industry forward.', 2, 3),\n",
        "    (4, 'A new vaccine has been developed to combat the virus.', 2, 4),\n",
        "    (5, 'The latest movie release has broken box office records.', 3, 5)\n",
        "])\n",
        "\n",
        "cursor.executemany('''\n",
        "INSERT INTO annotatorLabels (id, document_id, category_id, annotator_user_id) VALUES (?, ?, ?, ?)\n",
        "''', [\n",
        "    (1, 1, 1, 1),\n",
        "    (2, 2, 2, 2),\n",
        "    (3, 3, 3, 3),\n",
        "    (4, 4, 4, 4),\n",
        "    (5, 5, 5, 5)\n",
        "])\n",
        "\n",
        "conn.commit()\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "aAJdgVLEh48Z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_db_connection():\n",
        "    conn = sqlite3.connect('example_2.db')\n",
        "    return conn, conn.cursor()\n",
        "\n",
        "def fetch_data():\n",
        "    conn, cursor = get_db_connection()\n",
        "    query = '''\n",
        "        SELECT d.id as document_id, d.document_text, al.category_id, u.reliability\n",
        "        FROM documents d\n",
        "        JOIN annotatorLabels al ON d.id = al.document_id\n",
        "        JOIN users u ON al.annotator_user_id = u.id\n",
        "    '''\n",
        "    cursor.execute(query)\n",
        "    data = cursor.fetchall()\n",
        "    conn.close()\n",
        "    return data\n",
        "\n",
        "def compute_weighted_labels(df):\n",
        "    weighted_labels = df.groupby('document_id').apply(\n",
        "        lambda x: x.groupby('category_id')['reliability'].sum().idxmax()\n",
        "    ).reset_index(name='weighted_category_id')\n",
        "    return weighted_labels\n",
        "\n"
      ],
      "metadata": {
        "id": "pXIHt8iakOol"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = fetch_data()\n",
        "\n",
        "columns = ['document_id', 'document_text', 'category_id', 'reliability']\n",
        "df = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "weighted_labels = compute_weighted_labels(df)\n",
        "\n",
        "df = df.drop_duplicates(subset='document_id')\n",
        "\n",
        "df = df.merge(weighted_labels, on='document_id')\n",
        "\n",
        "documents = df['document_text'].tolist()\n",
        "labels = df['weighted_category_id'].tolist()"
      ],
      "metadata": {
        "id": "i5WQQ5Avo-1D"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_documents = [doc.split() for doc in documents]\n",
        "\n",
        "word2vec_model = Word2Vec(sentences=tokenized_documents, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "def document_vector(doc):\n",
        "    doc = [word for word in doc if word in word2vec_model.wv.key_to_index]\n",
        "    if len(doc) == 0:\n",
        "        return np.zeros(100)\n",
        "    return np.mean(word2vec_model.wv[doc], axis=0)\n",
        "\n",
        "X = np.array([document_vector(doc) for doc in tokenized_documents if len(doc) > 0])\n",
        "y = LabelEncoder().fit_transform(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "classifier = RandomForestClassifier(random_state=42)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "predicted_categories = classifier.predict(X)\n",
        "\n",
        "print(predicted_categories)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6t08Mm99okNF",
        "outputId": "25618d29-4137-4010-8103-88c339cbd0a4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 2 3 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def update_predicted_categories(document_ids, predicted_categories):\n",
        "    conn, cursor = get_db_connection()\n",
        "    for doc_id, category in zip(document_ids, predicted_categories):\n",
        "        cursor.execute('UPDATE documents SET predicted_category_id = ? WHERE id = ?', (int(category), int(doc_id)))\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "update_predicted_categories(df['document_id'].tolist(), predicted_categories)\n",
        "\n",
        "print(\"Database updated with predicted categories.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka2jej3wnAC-",
        "outputId": "451e51c9-5806-43e7-b48f-223ce91a0b57"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database updated with predicted categories.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def view_table(table_name):\n",
        "    conn = sqlite3.connect('example.db')\n",
        "    cursor = conn.cursor()\n",
        "    query = f\"SELECT * FROM {table_name}\"\n",
        "    cursor.execute(query)\n",
        "    rows = cursor.fetchall()\n",
        "    conn.close()\n",
        "    return rows\n",
        "\n"
      ],
      "metadata": {
        "id": "QvSeIu47kbXx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = view_table('documents')\n",
        "print(\"Documents Table:\")\n",
        "for row in documents:\n",
        "    print(row)\n",
        "\n",
        "\n",
        "categories = view_table('categories')\n",
        "print(\"\\nCategories Table:\")\n",
        "for row in categories:\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2qGxtG5o0FM",
        "outputId": "42be8435-cc15-40ca-914d-a62ed647e5bc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documents Table:\n",
            "(1, 'The football match was thrilling with a last-minute goal.', 1, 0)\n",
            "(2, 'The new policy has caused quite a stir among the citizens.', 1, 3)\n",
            "(3, 'Innovations in AI are driving the tech industry forward.', 2, 2)\n",
            "(4, 'A new vaccine has been developed to combat the virus.', 2, 3)\n",
            "(5, 'The latest movie release has broken box office records.', 3, 0)\n",
            "\n",
            "Categories Table:\n",
            "(1, 'Sports', 1)\n",
            "(2, 'Politics', 1)\n",
            "(3, 'Technology', 2)\n",
            "(4, 'Health', 2)\n",
            "(5, 'Entertainment', 3)\n"
          ]
        }
      ]
    }
  ]
}